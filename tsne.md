<!--
 FileName:      tsne
 Author:        8ucchiman
 CreatedDate:   2023-05-08 16:00:07
 LastModified:  2023-01-25 10:56:12 +0900
 Reference:     https://qiita.com/g-k/items/120f1cf85ff2ceae4aba
 Description:   ---
-->


# 背景
PCA, MDSは線形的な次元削減において欠点があった
1. 異なるデータを低次元上でも遠くに保つことに焦点を当てたアルゴリズムのため、類似しているデータを提示現状でも近くに保つことは難しい。
2. 高次元上の非線形的なデータにたいしては、類似しているデータを低次元上でも近くに保つことは不可能。


# t-sne
- 処理ポイント
  - 高次元での距離分布が低次元での距離分布にもできるだけ合致するように変換
  - 距離の分布をスチューデントのt-分布に従うと仮定

- pros
  - 高次元の局所的な構造を非常によくとらえる
  - 大局的な構造も可能な限りとらえる

- cons
  - perplexity(内部パラメータ)を変えると全く異なるクラスターが出現する。


# algorithm
## SNEの仕組み
### 1. データポイント間の距離を条件付確率に変換
まずは、データポイント間の距離を条件付確率に変換する。
$x_i$と$x_j$の類似度を、$x_i$が与えられたときに近傍として$x_j$を選択する条件付き確率$p_{j|i}$として表現する。
さらに、$x_j$は$x_i$を中心とした(つまり、平均$x_i$)正規分布に基づいて、確率的に選択されると仮定。
$
    p_{j|i} = 
        \frac{\frac{\exp{-(x_i-x_j)^2}}{2\sigma_i^2}}{\sum_{k\neq i}\frac{\exp{-(x_i-x_k)^2}}{2\sigma_i^2}}
$

### 2. 次元削減後のデータポイント間の距離も条件付確率で表現
次元削減後のデータポイント$y_i$と$y_j$の類似度も先ほどと同様に条件付確率$q_{j|i}$として表現する。
こちらも、$y_j$は$y_i$を中心とした正規分布に基づいて確率的に選択すると仮定する。
先ほどと異なるのは、分散は$\frac{1}{\sqrt{2}}$で固定する。
$
    q_{j|i} = 
        \frac{\exp{-(x_i-x_j)^2}{\sum_{k\neq i}\exp{-(x_i-x_k)^2}}
   
$

### 3. KLダイバージェンスで損失関数設計
`$p_{i|j}=q_{i|j}$の誤差を最小にすることを目指す`

高次元でのデータポイント間の距離関係と次元削減後の低次元での距離関係ができるだけ一致すればよいので$p_{i|j}=q_{i|j}$の誤差を最小にすることを目指す。
損失関数$C$は次のようにあらわされる。
$
    C = \sum_{i}{KL(P_i)(Q_i)} = \sum_{i}\sum_{j}p_{j|i}\ln{\frac{p_{j|i}}{q_{j|i}}}
$
KLダイバージェンスは非対称な指標なので$KL(P_i, Q_i)\neq KL(Q_i, P_i)$となる。
