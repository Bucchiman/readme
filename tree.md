<!--
 FileName:      tree
 Author:        8ucchiman
 CreatedDate:   2023-04-26 16:53:11
 LastModified:  2023-01-25 10:56:12 +0900
 Reference:     https://fresopiya.com/2019/06/10/desisiontreeid3/
 Description:   ---
-->


# Decision Tree
どのような質問をどの順番で行うか決める必要がある。
その時、代表的なアルゴリズムとして次のようなものがある。
- ID3(iterative dichotomiser 3)
- C4,5
- CART
- CHAID

## ID3(iterative dichotomiser 3)
`平均情報量からその中で最大のものを選び、それを木のノードにする操作を再帰的に行う`
`Impurity(不純度)をエントロピー(平均情報量)とする手法`
`母集団の平均情報量を求めた後、説明変数によって分けられた集団の各平均情報量を求め、母集団とその平均情報量の差分が最大のものの説明変数をノードとする。これを繰り返す。`
各独立変数に対し、変数の値を決定した場合における平均情報量の期待値を求める。
その中で最大のものを選び、それを木のノードにする操作を再帰的に行う。
- 0. 学習データ準備
- 1. ルートノードNを作成し、全データを所属させる。
- 2. Nに所属するデータがすべて同じ分類先なら、処理を終了。
- 3. Nに所属するデータのばらつき度合い(平均情報量)を計算。
- 4. 各変数を質問とした時のばらつき度合い(平均情報量)を求める。
- 5. 各変数を質問とした時の情報利得を計算。
- 6. 情報利得が最も大きい質問を選択し、分割後のデータを新たな集合とみなし、操作を繰り返す。


### 0. 学習データ準備
|         |   食性   | 発生形態 | 体温 |   分類    |
|---------|----------|----------|------|-----------|
| penguin |   肉食   |   卵生   | 恒温 | bird      |
|  lion   |   肉食   |   胎生   | 恒温 | mammalian |
|   cow   |   草食   |   胎生   | 恒温 | mammalian |
| lizard  |   肉食   |   卵生   | 変温 | reptiles  |
| chicken |   草食   |   卵生   | 恒温 | bird      |


### 1. ルートノードNを作成し、全データを所属させる。
```
   +-N-------------------+
   | *penguin ***lizard  |  - *bird
   | **lion     *chicken |  - **mammalian
   | **cow               |  - ***reptiles
   +---------------------+
```

### 2. Nに所属するデータがすべて同じ分類先なら、処理を終了。
pass

### 3. Nに所属するデータのばらつき度合い(平均情報量)を計算。
- 集合Cにおいて、D中のxが起こる確率を$p_x(C)$
```
   C: *penguin, **lion, **cow, ***lizard, *chicken
   D: *bird, **mammalian, ***reptiles
```

$ M(C) = -\sum_{x\in D} p_{x}(C)\log{p_{x}(C)} $
$ M(C) = -\frac{2}{5}\log_{3}{\frac{2}{5}} -\frac{2}{5}\log_{3}{\frac{2}{5}} -\frac{1}{5}\log_{3}{\frac{1}{5}} = 0.96 $

### 4. 各変数を質問とした時のばらつき度合い(平均情報量)を求める。

```
   +-N-------------------+  肉食or草食   +-N'--------+  +-N"--------+
   | *penguin ***lizard  | ----------->  | *penguin  |  | **cow     |
   | **lion     *chicken |               | **lion    |  | *chicken  |
   | **cow               |               | ***lizard |  +-----------+
   +---------------------+               +-----------+

                            卵生or胎生
                           ----------->  +-N'--------+  +-N"--------+
                                         | *penguin  |  | **cow     |
                                         | *chicken  |  | **lion    |
                                         | ***lizard |  +-----------+
                                         +-----------+

                            恒温or変温
                           ---------->   +-N'--------+  +-N"--------+
                                         | *penguin  |  | ***lizard |
                                         | *chicken  |  +-----------+
                                         | **lion    |
                                         | **cow     |
                                         +-----------+
```
 - 肉食or草食
    - 肉食の平均情報量
        $ -\frac{1}{3}\log_{3}{\frac{1}{3}} -\frac{1}{3}\log_{3}{\frac{1}{3}} -\frac{1}{3}\log_{3}{\frac{1}{3}} = 1.0 $
    - 草食の平均情報慮
        $ -\frac{1}{2}\log_{3}{\frac{1}{2}} -\frac{1}{2}\log_{3}{\frac{1}{2}} -\frac{0}{2}\log_{3}{\frac{0}{2}} = 0.631 $
    - 平均情報量の平均値
        $1.0\times\frac{3}{5} + 0.631\times\frac{2}{5} = 0.852 $

 - 卵生or胎生
    0.348

 - 恒温or変温
    0.505

平均情報量はデータのばらつきとして用いることができる。
肉食or草食ではbird, mammalian, reptilesがまんべんなく散らばっているため、平均情報量が最も大きくなっている。


### 5. 各変数を質問とした時の情報利得を計算。
どの質問を分岐に使ったらいいのかを判断するのに、平均情報量をもとに情報利得を用いる。
情報利得は、ある質問をしてデータを分割した結果、どれくらいエントロピーが減少したかを表す指標です。
もともとの平均情報量から、質問によるデータ分割後の平均情報量を引いた値であらわされる。
この値が大きければ、より有益な質問といえる。

- 肉食or草食
$ 0.96 - 0.852 = 0.108 $

- 卵生or胎生
$ 0.96 - 0.348 = 0.612 $

- 恒温or変温
$ 0.96 - 0.505 = 0.455 $

卵生か胎生かの質問の情報利得が最も大きいため、この質問が最も有益である。

### 6. 情報利得が最も大きい質問を選択し、分割後のデータを新たな集合とみなし、操作を繰り返す。

```
   +-N-------------------+  卵生or胎生   +-N'--------+  +-N"--------+
   | *penguin ***lizard  | ----------->  | *penguin  |  | **cow     |
   | **lion     *chicken |               | *chicken  |  | **lion    |
   | **cow               |               | ***lizard |  +-----------+
   +---------------------+               +-----------+
                                               |              |
                                               v              v
                                          繰り返し処理    処理終了

```


## CART
`ジニ不純度(Gini Impurity)`
$ Gini = \sum_x{p(x)(1-p(x))} $
2つの事象の出現確率がそれぞれ0, 1だった場合、Gini=0となる。
一方でどちらも$\frac{1}{2}$である場合、$Gini=\frac{1}{2}$なので、出現率に偏りがなくなるほどジニ不純度が増大する。



## アルゴリズム別理解
 - ID3
 - C4,5
 - CART
 - CHAID




# アンサンブル学習
[Ref](https://www.stats-guild.com/analytics/12869)
[Ref](https://www.knowledgehut.com/blog/data-science/bagging-and-random-forest-in-machine-learning)
`複数のモデルを使用した集団学習` `モデルのバイアスとバリアンスを小さくする`
- アンサンブル学習
    - バギング
        - random forest
    - ブースティング
        - 勾配ぶースティング
        - adaboost

## バギング
`モデルの予測結果のバリアンスを小さくする(すなわち過学習を防ぐ)`
`一つの学習データを小分けにして(ブートストラップサンプリング)バラエティにとんだ学習モデルを複数用意して、その平均を採る手法`
`過学習気味の自由度の高いモデルを複数用意`
`弱学習器`

ブートストラップとはデータセットを作成する手法。
データ$(d_1, d_2, d_3, d_4, d_5)$に対して、決定木1では$(d_1, d_2, d_3, d_4)$、決定木2では$(d_1, d_3, d_4, d_5)$とする。

```
           +---------------------+
          <       学習データ      >
           +---------------------+
          /           |           \
         /            |            \
 +-----------+  +-----------+  +-----------+
< 学習データA >< 学習データB >< 学習データC >
 +-----------+  +-----------+  +-----------+
       |              |             |
 +-----------+  +-----------+  +-----------+
 |  モデル1  |  |  モデル2  |  |  モデル3  |
 +-----------+  +-----------+  +-----------+
         \            |            /
          \           |           /
           +---------------------+
           |       モデル        |
           +---------------------+

```

### random forest
`学習データは層下抽出で選択するため、それぞれの決定木は異なるデータで学習`
`バギングの一種, バリアンスを小さくする`
`バギングとの違いは、説明変数と特徴量の両方をサンプリングしている点`
決定木は過学習と精度を上げることはトレードオフで両立は難しい。
そこで1つの木で精度を上げることを考えるのではなく、多くの決定木を組み合わせたrandom forest手法が考案された。

バギングでは同じ説明変数を使用してモデルを作成するが、ランダムフォレストでは説明変数もサンプリングする。

1. ブートストラップサンプリングにより複数の学習データセットを生成
2. 学習データ節とごとにランダムにK個の説明変数を選択し、決定木をN個作成
3. N個の決定木の結果をアンサンブル

#### hyperparameter
- 作成するツリー数
- 説明変数の数

#### メリット
- 特徴量の重要度が評価できる
- バリアンスが小さい(すなわち過学習が起きにくい)
- 複数のツリーの並列処理が可能


## ブースティング
`モデルのバイアス(分散)を小さくする`
`モデルを1つずつ順番に作成しご分類したデータを優先的に充てるようにデータセットに重みをつけて調整する`
`前のモデルの結果を考慮して重みを調整するため、並列処理できない`

```
           +---------------------+
          <       学習データ      >
           +---------------------+
          /           |           \
         /            |            \
 +-----------+  +-----------+  +-----------+
< 学習データA >< 学習データB >< 学習データC >
 +-----------+  +-----------+  +-----------+
       |      /重み   |      /重み  |
 +-----------+  +-----------+  +-----------+
 |  モデル1  |  |  モデル2  |  |  モデル3  |
 +-----------+  +-----------+  +-----------+
         \            |            /
          \           |           /
           +---------------------+
           |       モデル        |
           +---------------------+

```



- 勾配ブースティング
- adaboost

### 勾配ブースティング
`弱学習器であっても全体の予測精度が高精度になる`



### adaboost
正しく識別できるサンプルは重みが小さく、ご識別したサンプルの重みは大きくなる。

#### アルゴリズム
1. 学習サンプルの重みの初期化
$D = \sum_{h(x)\neq D}$

2. 各弱識別器候補に対して,エラー率$\epsilon$を算出、そのエラー率が最も小さな弱識別器を選択する。

3. エラー率から弱識別機の重みを計算する。エラー率が小さいほど弱識別器の重みは大きくなる。
$\alpha=\frac{1}{2}\ln{\frac{1-\epsilon}{\epsilon}}$

4. 学習サンプルの重み$D$を更新する。
正解サンプルはクラスラベルと同符号となる。
エラー率が小さいほど弱識別器の重みが大きいため、

# LightGBM(Light Gradient Boosting Machine)
[Ref](https://datawokagaku.com/lightgbm/)
`決定木の勾配ブースティング`

