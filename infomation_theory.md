<!--
 FileName:      infomation_theory
 Author:        8ucchiman
 CreatedDate:   2023-04-26 17:46:52
 LastModified:  2023-01-25 10:56:12 +0900
 Reference:     https://logics-of-blue.com/information-theory-basic/
 Description:   ---
-->


# 情報理論の直感的理解
- 1. 情報量を定義する
    - 1.1 性質1 珍しいことは情報量が多い
    - 1.2 性質2 情報は足し合わせ
    - 1.3 定義
- 2. 情報エントロピーと平均情報量
- 3. 相対エントロピー
- 4. 相互情報量


## 1. 情報量を定義
### 1.1 性質1 珍しいことは情報量が多い
ルーレットを例とする。
1~40まで割り振られている。


### 1.3 情報量の定義
以下の2つの要請を満たす情報量を定義する。
1. 発生する確率が低いことが分かった時の方が、情報量が多い。
2. 情報量は足し算で増えていく。

ここから情報量を定義する。
$ i(x) = - \log_{2}{P(X)} $
マイナスがついているのは「確率が小さくなるほど情報量は大きくなる」ということ。




## 交差エントロピー
`正解値と推定値の比較を行う``$H(p, q)=-p(x)\log{q(x)}$ (真の確率分布$p(x)$, 推定した確率分布$q(x)$)`


# klダイバージェンス
`2つの確率分布の違いを数量化したもの`

交差エントロピーと情報エントロピーの差分
$
    D_{KL}(P|Q) = H(P, Q) - H(P)
$

$H(P, Q)=E[-\log{Q}]$
$H(P)=E[-\log(P)]$
交差エントロピー


# 情報量
起こる確率$n$とした時、次のように定義される。
$\log_{2}{\frac{1}{n}}$

# 平均情報量(エントロピー)
情報量の期待値ともいえる
$-\sum_{i=1}^np(i)\log_{2}{p(i)}$
